{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "# from transformers import GPTNeoXForCausalLM\n",
    "from gpt_neox_mlkv import GPTNeoXConfig, GPTNeoXForCausalLM\n",
    "# from gpt_neox import GPTNeoXConfig, GPTNeoXForCausalLM\n",
    "# check for cuda\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "# load model\n",
    "tokenizer = AutoTokenizer.from_pretrained('../pythia-160m-deduped_mlkv_6_6')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('../pythia-160m-deduped')\n",
    "# config = GPTNeoXConfig.from_pretrained('../pythia-160m-deduped')\n",
    "model = GPTNeoXForCausalLM.from_pretrained('../pythia-160m-deduped_mlkv_6_6')\n",
    "# model = GPTNeoXForCausalLM.from_pretrained('../pythia-160m-deduped')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-160m-deduped')\n",
    "# model = GPTNeoXForCausalLM.from_pretrained('EleutherAI/pythia-160m-deduped')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khalid Zuhri\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The scaling of language models to tens even hundreds of billions of parameters made possible by the highly parallelizable transformer architecture has led to the rise of large language models (LLMs) as the pinnacle of natural language processing and language inference \\parencite{pope2022efficiently}. Aside from achieving state-of-the-art in various benchmarks ranging from writing code to common sense and reasoning, LLMs have also been deployed massively at scale for applications such as OpenAI's ChatGPT or their API of various models \\parencite{openai2023gpt4}. But serving billion-scale models to millions of customers becomes a major challenge, requiring massive compute to fulfil the needed latency constraints. Aside from a small amount reserved for computation, memory is excessively needed for two main things. Firstly the model weights, which can be large but stay fixed once deployed. The more concerning memory consumer is the key-value cache (KV cache), which contains the stored activations of attention modules at all previous time steps. This is the result of optimizing generative inference: tokens are generated one by one, auto-regressively. To avoid unnecessary re-computation of these activations, they are stored in memory to be used for generating future tokens. At small scales, KV cache can be unassuming, and memory is mostly needed for the weights. But taking OPT-175B as a large scale example, its 175 billion parameters require 325GB of memory. Yet when inferencing a sequence length of 2048 and computing a batch of 128 generations at once, the KV cache can take up to 950GB of memory \\parencite{liu2023scissorhands}. In this case, the cache is three times larger than the model weights, yet even then, KV cache can still grow indefinitely in relation to the sequence length, batch size, and model dimension. TL;DR: inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent inconsistent\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The scaling of language models to tens even hundreds of billions of parameters made possible by the highly parallelizable transformer architecture has led to the rise of large language models (LLMs) as the pinnacle of natural language processing and language inference \\parencite{pope2022efficiently}. Aside from achieving state-of-the-art in various benchmarks ranging from writing code to common sense and reasoning, LLMs have also been deployed massively at scale for applications such as OpenAI's ChatGPT or their API of various models \\parencite{openai2023gpt4}. But serving billion-scale models to millions of customers becomes a major challenge, requiring massive compute to fulfil the needed latency constraints. Aside from a small amount reserved for computation, memory is excessively needed for two main things. Firstly the model weights, which can be large but stay fixed once deployed. The more concerning memory consumer is the key-value cache (KV cache), which contains the stored activations of attention modules at all previous time steps. This is the result of optimizing generative inference: tokens are generated one by one, auto-regressively. To avoid unnecessary re-computation of these activations, they are stored in memory to be used for generating future tokens. At small scales, KV cache can be unassuming, and memory is mostly needed for the weights. But taking OPT-175B as a large scale example, its 175 billion parameters require 325GB of memory. Yet when inferencing a sequence length of 2048 and computing a batch of 128 generations at once, the KV cache can take up to 950GB of memory \\parencite{liu2023scissorhands}. In this case, the cache is three times larger than the model weights, yet even then, KV cache can still grow indefinitely in relation to the sequence length, batch size, and model dimension. TL;DR:\"\"\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=1000, temperature=0.5)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.embed_in.weight\n",
      "gpt_neox.layers.0.input_layernorm.weight\n",
      "gpt_neox.layers.0.input_layernorm.bias\n",
      "gpt_neox.layers.0.post_attention_layernorm.weight\n",
      "gpt_neox.layers.0.post_attention_layernorm.bias\n",
      "gpt_neox.layers.0.attention.bias\n",
      "gpt_neox.layers.0.attention.masked_bias\n",
      "gpt_neox.layers.0.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.0.attention.query_key_value.weight\n",
      "gpt_neox.layers.0.attention.query_key_value.bias\n",
      "gpt_neox.layers.0.attention.dense.weight\n",
      "gpt_neox.layers.0.attention.dense.bias\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.1.input_layernorm.weight\n",
      "gpt_neox.layers.1.input_layernorm.bias\n",
      "gpt_neox.layers.1.post_attention_layernorm.weight\n",
      "gpt_neox.layers.1.post_attention_layernorm.bias\n",
      "gpt_neox.layers.1.attention.bias\n",
      "gpt_neox.layers.1.attention.masked_bias\n",
      "gpt_neox.layers.1.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.1.attention.query_key_value.weight\n",
      "gpt_neox.layers.1.attention.query_key_value.bias\n",
      "gpt_neox.layers.1.attention.dense.weight\n",
      "gpt_neox.layers.1.attention.dense.bias\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.2.input_layernorm.weight\n",
      "gpt_neox.layers.2.input_layernorm.bias\n",
      "gpt_neox.layers.2.post_attention_layernorm.weight\n",
      "gpt_neox.layers.2.post_attention_layernorm.bias\n",
      "gpt_neox.layers.2.attention.bias\n",
      "gpt_neox.layers.2.attention.masked_bias\n",
      "gpt_neox.layers.2.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.2.attention.query_key_value.weight\n",
      "gpt_neox.layers.2.attention.query_key_value.bias\n",
      "gpt_neox.layers.2.attention.dense.weight\n",
      "gpt_neox.layers.2.attention.dense.bias\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.3.input_layernorm.weight\n",
      "gpt_neox.layers.3.input_layernorm.bias\n",
      "gpt_neox.layers.3.post_attention_layernorm.weight\n",
      "gpt_neox.layers.3.post_attention_layernorm.bias\n",
      "gpt_neox.layers.3.attention.bias\n",
      "gpt_neox.layers.3.attention.masked_bias\n",
      "gpt_neox.layers.3.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.3.attention.query_key_value.weight\n",
      "gpt_neox.layers.3.attention.query_key_value.bias\n",
      "gpt_neox.layers.3.attention.dense.weight\n",
      "gpt_neox.layers.3.attention.dense.bias\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.4.input_layernorm.weight\n",
      "gpt_neox.layers.4.input_layernorm.bias\n",
      "gpt_neox.layers.4.post_attention_layernorm.weight\n",
      "gpt_neox.layers.4.post_attention_layernorm.bias\n",
      "gpt_neox.layers.4.attention.bias\n",
      "gpt_neox.layers.4.attention.masked_bias\n",
      "gpt_neox.layers.4.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.4.attention.query_key_value.weight\n",
      "gpt_neox.layers.4.attention.query_key_value.bias\n",
      "gpt_neox.layers.4.attention.dense.weight\n",
      "gpt_neox.layers.4.attention.dense.bias\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.5.input_layernorm.weight\n",
      "gpt_neox.layers.5.input_layernorm.bias\n",
      "gpt_neox.layers.5.post_attention_layernorm.weight\n",
      "gpt_neox.layers.5.post_attention_layernorm.bias\n",
      "gpt_neox.layers.5.attention.bias\n",
      "gpt_neox.layers.5.attention.masked_bias\n",
      "gpt_neox.layers.5.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.5.attention.query_key_value.weight\n",
      "gpt_neox.layers.5.attention.query_key_value.bias\n",
      "gpt_neox.layers.5.attention.dense.weight\n",
      "gpt_neox.layers.5.attention.dense.bias\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.6.input_layernorm.weight\n",
      "gpt_neox.layers.6.input_layernorm.bias\n",
      "gpt_neox.layers.6.post_attention_layernorm.weight\n",
      "gpt_neox.layers.6.post_attention_layernorm.bias\n",
      "gpt_neox.layers.6.attention.bias\n",
      "gpt_neox.layers.6.attention.masked_bias\n",
      "gpt_neox.layers.6.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.6.attention.query_key_value.weight\n",
      "gpt_neox.layers.6.attention.query_key_value.bias\n",
      "gpt_neox.layers.6.attention.dense.weight\n",
      "gpt_neox.layers.6.attention.dense.bias\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.7.input_layernorm.weight\n",
      "gpt_neox.layers.7.input_layernorm.bias\n",
      "gpt_neox.layers.7.post_attention_layernorm.weight\n",
      "gpt_neox.layers.7.post_attention_layernorm.bias\n",
      "gpt_neox.layers.7.attention.bias\n",
      "gpt_neox.layers.7.attention.masked_bias\n",
      "gpt_neox.layers.7.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.7.attention.query_key_value.weight\n",
      "gpt_neox.layers.7.attention.query_key_value.bias\n",
      "gpt_neox.layers.7.attention.dense.weight\n",
      "gpt_neox.layers.7.attention.dense.bias\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.8.input_layernorm.weight\n",
      "gpt_neox.layers.8.input_layernorm.bias\n",
      "gpt_neox.layers.8.post_attention_layernorm.weight\n",
      "gpt_neox.layers.8.post_attention_layernorm.bias\n",
      "gpt_neox.layers.8.attention.bias\n",
      "gpt_neox.layers.8.attention.masked_bias\n",
      "gpt_neox.layers.8.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.8.attention.query_key_value.weight\n",
      "gpt_neox.layers.8.attention.query_key_value.bias\n",
      "gpt_neox.layers.8.attention.dense.weight\n",
      "gpt_neox.layers.8.attention.dense.bias\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.9.input_layernorm.weight\n",
      "gpt_neox.layers.9.input_layernorm.bias\n",
      "gpt_neox.layers.9.post_attention_layernorm.weight\n",
      "gpt_neox.layers.9.post_attention_layernorm.bias\n",
      "gpt_neox.layers.9.attention.bias\n",
      "gpt_neox.layers.9.attention.masked_bias\n",
      "gpt_neox.layers.9.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.9.attention.query_key_value.weight\n",
      "gpt_neox.layers.9.attention.query_key_value.bias\n",
      "gpt_neox.layers.9.attention.dense.weight\n",
      "gpt_neox.layers.9.attention.dense.bias\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.10.input_layernorm.weight\n",
      "gpt_neox.layers.10.input_layernorm.bias\n",
      "gpt_neox.layers.10.post_attention_layernorm.weight\n",
      "gpt_neox.layers.10.post_attention_layernorm.bias\n",
      "gpt_neox.layers.10.attention.bias\n",
      "gpt_neox.layers.10.attention.masked_bias\n",
      "gpt_neox.layers.10.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.10.attention.query_key_value.weight\n",
      "gpt_neox.layers.10.attention.query_key_value.bias\n",
      "gpt_neox.layers.10.attention.dense.weight\n",
      "gpt_neox.layers.10.attention.dense.bias\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.layers.11.input_layernorm.weight\n",
      "gpt_neox.layers.11.input_layernorm.bias\n",
      "gpt_neox.layers.11.post_attention_layernorm.weight\n",
      "gpt_neox.layers.11.post_attention_layernorm.bias\n",
      "gpt_neox.layers.11.attention.bias\n",
      "gpt_neox.layers.11.attention.masked_bias\n",
      "gpt_neox.layers.11.attention.rotary_emb.inv_freq\n",
      "gpt_neox.layers.11.attention.query_key_value.weight\n",
      "gpt_neox.layers.11.attention.query_key_value.bias\n",
      "gpt_neox.layers.11.attention.dense.weight\n",
      "gpt_neox.layers.11.attention.dense.bias\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
      "gpt_neox.final_layer_norm.weight\n",
      "gpt_neox.final_layer_norm.bias\n",
      "embed_out.weight\n"
     ]
    }
   ],
   "source": [
    "# inspect pytorch_model.bin and see all the keys\n",
    "state_dict = torch.load('../pythia-160m-deduped/pytorch_model.bin', map_location='cpu')\n",
    "for k in state_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['gpt_neox.layers.11.attention.query_key_value.weight'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
